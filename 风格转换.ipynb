{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdb4cc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23ba7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7aa79c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具函数，从给定地址加载图片\n",
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "505d50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具函数，将图片显示出来\n",
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f2089",
   "metadata": {},
   "source": [
    "本次我们使用google网站上的两张图片来演示风格转换。下面的content_path表示内容图片的地址，style_path指风格图片的地址。\n",
    "由于后面不少代码都要联网，所以有时候代码执行会花一些时间，并且有时候因为网络问题会导致执行失败，多试几次就能成功的！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6162a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "content_path = \"C:/Users/CXY/Desktop/111.jpg\"\n",
    "style_path = \"C:Users/CXY/Desktop/222.jpg\"\n",
    "\n",
    "# style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2660e",
   "metadata": {},
   "source": [
    "下面的代码会将内容图片和风格图片加载进来并且显示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb27f537",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 168: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m content_image \u001B[38;5;241m=\u001B[39m \u001B[43mload_img\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m style_image \u001B[38;5;241m=\u001B[39m load_img(style_path)\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[28], line 4\u001B[0m, in \u001B[0;36mload_img\u001B[1;34m(path_to_img)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_img\u001B[39m(path_to_img):\n\u001B[0;32m      3\u001B[0m   max_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m512\u001B[39m\n\u001B[1;32m----> 4\u001B[0m   img \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_to_img\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m   img \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mdecode_image(img, channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m      6\u001B[0m   img \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mconvert_image_dtype(img, tf\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[1;32mD:\\software\\miniconda\\envs\\cxy\\lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py:133\u001B[0m, in \u001B[0;36mread_file\u001B[1;34m(filename, name)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mio.read_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mio.read_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_file\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_file\u001B[39m(filename, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     98\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Reads the contents of file.\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \n\u001B[0;32m    100\u001B[0m \u001B[38;5;124;03m  This operation returns a tensor with the entire contents of the input\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    A tensor of dtype \"string\", with the file contents.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgen_io_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\software\\miniconda\\envs\\cxy\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:610\u001B[0m, in \u001B[0;36mread_file\u001B[1;34m(filename, name)\u001B[0m\n\u001B[0;32m    608\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 610\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mread_file_eager_fallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m      \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_ctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_SymbolicException:\n\u001B[0;32m    613\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\software\\miniconda\\envs\\cxy\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:633\u001B[0m, in \u001B[0;36mread_file_eager_fallback\u001B[1;34m(filename, name, ctx)\u001B[0m\n\u001B[0;32m    631\u001B[0m _inputs_flat \u001B[38;5;241m=\u001B[39m [filename]\n\u001B[0;32m    632\u001B[0m _attrs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m _result \u001B[38;5;241m=\u001B[39m \u001B[43m_execute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mReadFile\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_inputs_flat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_attrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _execute\u001B[38;5;241m.\u001B[39mmust_record_gradient():\n\u001B[0;32m    636\u001B[0m   _execute\u001B[38;5;241m.\u001B[39mrecord_gradient(\n\u001B[0;32m    637\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReadFile\u001B[39m\u001B[38;5;124m\"\u001B[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001B[1;32mD:\\software\\miniconda\\envs\\cxy\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xd5 in position 168: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fa758",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个工具函数，将tensor转换为图片。并且显示出来\n",
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要先安装pip install tensorflow-hub-0.12.0\n",
    "import tensorflow_hub as hub\n",
    "hub_model = hub.load('https://hub.tensorflow.google.cn/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
    "tensor_to_image(stylized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc185b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg就代表了一个训练好了的VGG19模型\n",
    "# include_top=False代表不需要最后一层。因为咱们只用它来风格转换，不需要最后一层，最后一层是用来识别图片的。\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "# 将每层的名字打印出来\n",
    "for layer in vgg.layers:\n",
    "  print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767abab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0750258",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5df629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个自定义的模型\n",
    "# 在输入vgg.input后（也就是一张图片后），这个函数会返回上面定义的那些网络层的激活值。\n",
    "def vgg_layers(layer_names):\n",
    "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "\n",
    "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "  model = tf.keras.Model([vgg.input], outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0ab38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "  input_shape = tf.shape(input_tensor)\n",
    "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "  return result/(num_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda7868",
   "metadata": {},
   "source": [
    "下面我们用subclassing的方式定义了一个类。\n",
    "\n",
    "这个类整合了上面那些工具函数。\n",
    "\n",
    "当将一张图片输入后，这个类会返回这个图片的内容激活值矩阵以及风格矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2665840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "  def __init__(self, style_layers, content_layers):\n",
    "    super(StyleContentModel, self).__init__()\n",
    "    self.vgg = vgg_layers(style_layers + content_layers)\n",
    "    self.style_layers = style_layers\n",
    "    self.content_layers = content_layers\n",
    "    self.num_style_layers = len(style_layers)\n",
    "    self.vgg.trainable = False \n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"Expects float input in [0,1]\"\n",
    "    inputs = inputs*255.0\n",
    "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "    outputs = self.vgg(preprocessed_input)\n",
    "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                      outputs[self.num_style_layers:])\n",
    "\n",
    "    style_outputs = [gram_matrix(style_output)\n",
    "                     for style_output in style_outputs]\n",
    "\n",
    "    content_dict = {content_name: value\n",
    "                    for content_name, value\n",
    "                    in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "    style_dict = {style_name: value\n",
    "                  for style_name, value\n",
    "                  in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "    return {'content': content_dict, 'style': style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style'] # 获取风格图片的风格矩阵\n",
    "content_targets = extractor(content_image)['content'] # 获取内容图片的内容激活值矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501730d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制内容图片到image。\n",
    "# 后面会不断的根据content_image和style_image来改变image，时image的风格越来越像style_image\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59616f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个工具函数，用来修剪图片的数值\n",
    "def clip_0_1(image):\n",
    "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight=1e-2 # 可以通过这个值来控制风格化到什么程度\n",
    "content_weight=1e4 # 控制内容保留的程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用这个函数来对比最终图片image与内容图片content_image风格图片style_image的差别。\n",
    "# 也就是我们所说的损失函数。差别越大损失就越大。\n",
    "# 当iamge的内容越来越像content_image，风格越来越像style_image，那么损失就会越来越小。\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style'] # image当前的风格矩阵\n",
    "    content_outputs = outputs['content'] # image当前的内容激活值矩阵\n",
    "    \n",
    "    # 计算风格损失\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    # 计算内容损失\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    \n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b9591",
   "metadata": {},
   "source": [
    "下面的函数定义了一步完整的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77dc569",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "  # tape会记录下前向传播的每一个步骤，后面好自动执行反向传播\n",
    "  with tf.GradientTape() as tape:\n",
    "    outputs = extractor(image) # 获取当前image的内容激活值矩阵和风格矩阵\n",
    "    loss = style_content_loss(outputs) # 计算损失。即与内容图片content_image风格图片style_image的差距\n",
    "\n",
    "  # 获取image相对于loss的梯度。这里的image就相当于w和b参数一样。\n",
    "  grad = tape.gradient(loss, image)\n",
    "  # 使用梯度来改变image，也即是说image会变得越来越像content_image风格图片style_image\n",
    "  opt.apply_gradients([(grad, image)]) \n",
    "  \n",
    "  image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面我们只训练3步。从结果可以看出，图片已经有了一点点风格图片的感觉了\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "tensor_to_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a27dffb",
   "metadata": {},
   "source": [
    "下面的代码就是训练很多步。注意哦，会花很长时间，电脑配置不好的可能要花几个小时，甚至更长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12caeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "  for m in range(steps_per_epoch):\n",
    "    step += 1\n",
    "    train_step(image)\n",
    "    print(\".\", end='', flush=True)\n",
    "  display.clear_output(wait=True)\n",
    "  display.display(tensor_to_image(image))\n",
    "  print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
